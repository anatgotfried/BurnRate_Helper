#!/usr/bin/env python3
"""
Score Fast Mode comparison with GPT-4o
Compares quality of Fast ON vs Fast OFF for each model
"""

import json
import requests
import glob
from dotenv import load_dotenv
import os
import time

load_dotenv()

API_KEY = os.getenv('OPENROUTER_API_KEY')
API_URL = 'https://openrouter.ai/api/v1/chat/completions'

def compare_with_gpt4o(model_name, fast_response, full_response):
    """Have GPT-4o compare Fast vs Full for one model"""
    
    prompt = f"""Compare these two meal plans generated by the same model ({model_name}).
One used FILTERED corpus (~5k tokens), one used FULL corpus (~15k tokens).

FAST MODE (Filtered Corpus):
{json.dumps(fast_response, indent=2)[:3000]}...

FULL MODE (Full Corpus):
{json.dumps(full_response, indent=2)[:3000]}...

Rate each on 1-10 scale and determine if Fast Mode sacrificed quality:

Return ONLY this JSON:
{{
  "model": "{model_name}",
  "fast_mode_score": N,
  "full_mode_score": N,
  "fast_meal_count": N,
  "full_meal_count": N,
  "quality_difference": "none|minor|significant",
  "fast_mode_drawbacks": "What was lost with filtering (if any)",
  "recommendation": "use_fast|use_full|either_ok",
  "reasoning": "2-3 sentence explanation"
}}"""
    
    try:
        response = requests.post(
            API_URL,
            headers={
                'Authorization': f'Bearer {API_KEY}',
                'Content-Type': 'application/json'
            },
            json={
                'model': 'openai/gpt-4o',
                'messages': [
                    {'role': 'system', 'content': 'You are an expert evaluator. Return ONLY valid JSON.'},
                    {'role': 'user', 'content': prompt}
                ],
                'max_tokens': 1000,
                'temperature': 0.2
            },
            timeout=30
        )
        
        if response.status_code == 200:
            data = response.json()
            content = data['choices'][0]['message']['content']
            
            # Extract JSON
            if '```json' in content:
                content = content.split('```json')[1].split('```')[0].strip()
            first_brace = content.find('{')
            last_brace = content.rfind('}')
            if first_brace != -1:
                content = content[first_brace:last_brace + 1]
            
            return json.loads(content)
    except:
        pass
    
    return {
        'model': model_name,
        'fast_mode_score': 0,
        'full_mode_score': 0,
        'recommendation': 'evaluation_failed'
    }

def main():
    print('\n' + '='*80)
    print('üìä GPT-4o Comparison: Fast Mode ON vs OFF')
    print('='*80 + '\n')
    
    fast_files = glob.glob('testing/test3_fast_on/*.json')
    comparisons = []
    
    for fast_file in fast_files:
        model_id = os.path.basename(fast_file).replace('.json', '')
        full_file = fast_file.replace('fast_on', 'fast_off')
        
        if not os.path.exists(full_file):
            continue
        
        print(f'Comparing {model_id}...')
        
        with open(fast_file, 'r') as f:
            fast_data = json.load(f)
        with open(full_file, 'r') as f:
            full_data = json.load(f)
        
        comparison = compare_with_gpt4o(
            model_id,
            fast_data['response'],
            full_data['response']
        )
        
        comparisons.append(comparison)
        
        print(f'  Fast: {comparison.get("fast_mode_score", 0)}/10')
        print(f'  Full: {comparison.get("full_mode_score", 0)}/10')
        print(f'  ‚Üí {comparison.get("recommendation", "unknown")}')
        
        time.sleep(2)
    
    # Save
    with open('testing/scores/fast-mode-comparison.json', 'w') as f:
        json.dump({
            'comparisons': comparisons,
            'scored_by': 'openai/gpt-4o'
        }, f, indent=2)
    
    # Report
    print('\n' + '='*80)
    print('FAST MODE IMPACT ANALYSIS')
    print('='*80 + '\n')
    
    for c in comparisons:
        print(f'{c.get("model", "unknown")}:')
        print(f'  Quality impact: {c.get("quality_difference", "unknown")}')
        print(f'  Recommendation: {c.get("recommendation", "unknown")}')
        print(f'  Reasoning: {c.get("reasoning", "N/A")[:80]}')
        print()
    
    print('‚úÖ Comparison complete!')
    print('üìÅ Saved: testing/scores/fast-mode-comparison.json')

if __name__ == '__main__':
    main()

